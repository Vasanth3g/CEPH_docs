proper remove:

ceph osd purge {id} --yes-i-really-mean-it

ceph osd crush remove {name}

ceph auth del osd.{osd-num}

ceph osd rm {osd-num}

Replacing :(must need uuid) 

ceph osd destroy {id} --yes-i-really-mean-it

ceph-disk zap /dev/sdX

ceph-disk prepare --bluestore /dev/sdX  --osd-id {id} --osd-uuid `uuidgen`

ceph-disk activate /dev/sdX1

Adding osd :

ceph-deploy osd create --data /dev/sdX {host} (automation )

ceph osd create [{uuid} [{id}]]

ssh {new-osd-host}
sudo mkdir /var/lib/ceph/osd/ceph-{osd-number}

ssh {new-osd-host}
sudo mkfs -t {fstype} /dev/{drive}
sudo mount -a  /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}

ssh {new-osd-host}
ceph-osd -i {osd-num} --mkfs --mkkey


ceph auth add osd.{osd-num} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-num}/keyring

ceph osd crush add {id-or-name} {weight}  [{bucket-type}={bucket-name} ...]

Starting osd :

Systemctl start ceph-osd@{id}  #####start , stop , status

